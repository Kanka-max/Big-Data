{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4d286bb-ef4d-45aa-9219-11090712f5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/macbook/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/15 10:06:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/macbook/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d89cd43-5949-4604-a7c9-d7938664a961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f540b154-0743-42a5-8f9b-dfbe8c799ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|      Category|            Descript|\n",
      "+--------------+--------------------+\n",
      "|      WARRANTS|      WARRANT ARREST|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|\n",
      "+--------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Remove the columns we do not need and have a look the first five rows:\n",
    "\n",
    "drop_list = ['Dates', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
    "data = data.select([column for column in data.columns if column not in drop_list])\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d84f0a4-f87a-48e9-a4b8-d0492dfaa533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 crime categories:\n",
    "from pyspark.sql.functions import col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ac32662-fff2-4146-ac8d-eb3da7cc2bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            Category| count|\n",
      "+--------------------+------+\n",
      "|       LARCENY/THEFT|174900|\n",
      "|      OTHER OFFENSES|126182|\n",
      "|        NON-CRIMINAL| 92304|\n",
      "|             ASSAULT| 76876|\n",
      "|       DRUG/NARCOTIC| 53971|\n",
      "|       VEHICLE THEFT| 53781|\n",
      "|           VANDALISM| 44725|\n",
      "|            WARRANTS| 42214|\n",
      "|            BURGLARY| 36755|\n",
      "|      SUSPICIOUS OCC| 31414|\n",
      "|      MISSING PERSON| 25989|\n",
      "|             ROBBERY| 23000|\n",
      "|               FRAUD| 16679|\n",
      "|FORGERY/COUNTERFE...| 10609|\n",
      "|     SECONDARY CODES|  9985|\n",
      "|         WEAPON LAWS|  8555|\n",
      "|        PROSTITUTION|  7484|\n",
      "|            TRESPASS|  7326|\n",
      "|     STOLEN PROPERTY|  4540|\n",
      "|SEX OFFENSES FORC...|  4388|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data.groupBy(\"Category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a0da681-6a63-4119-aefb-18e9855abc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            Descript|count|\n",
      "+--------------------+-----+\n",
      "|GRAND THEFT FROM ...|60022|\n",
      "|       LOST PROPERTY|31729|\n",
      "|             BATTERY|27441|\n",
      "|   STOLEN AUTOMOBILE|26897|\n",
      "|DRIVERS LICENSE, ...|26839|\n",
      "|      WARRANT ARREST|23754|\n",
      "|SUSPICIOUS OCCURR...|21891|\n",
      "|AIDED CASE, MENTA...|21497|\n",
      "|PETTY THEFT FROM ...|19771|\n",
      "|MALICIOUS MISCHIE...|17789|\n",
      "|   TRAFFIC VIOLATION|16471|\n",
      "|PETTY THEFT OF PR...|16196|\n",
      "|MALICIOUS MISCHIE...|15957|\n",
      "|THREATS AGAINST LIFE|14716|\n",
      "|      FOUND PROPERTY|12146|\n",
      "|ENROUTE TO OUTSID...|11470|\n",
      "|GRAND THEFT OF PR...|11010|\n",
      "|POSSESSION OF NAR...|10050|\n",
      "|PETTY THEFT FROM ...|10029|\n",
      "|PETTY THEFT SHOPL...| 9571|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Top 20 crime descriptions:\n",
    "\n",
    "data.groupBy(\"Descript\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4cae92-e7a6-477c-93c7-2b10da41d258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b6689de-7d67-4c4f-b9bb-0c8dab68d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regular expression tokenizer\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"Descript\", outputCol=\"words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5946100d-8f61-4d82-aea8-5bb19afc569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words\n",
    "\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68ce1c89-e181-466e-b427-def111ccad51",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5286d-9eec-4c91-9158-60ab7209eca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bc8be95-75cd-4396-aa54-ac9c95e77608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the string column of labels to a column of label indices\n",
    "\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "label_string_index = StringIndexer(inputCol = \"Category\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd59b22-441e-4a46-ae00-9b66740e751e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9dcd73b8-fa9d-4f7e-a627-fae0d709775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "# Logistic Regression using Word2Vec Features\n",
    "# Transforms a word into a code for machine learning process.\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2_vec = Word2Vec(inputCol=\"filtered\", outputCol=\"rawFeatures\", seed= 42)\n",
    "\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwords_remover, word2_vec, label_string_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5715d0be-1aa9-4002-9ddb-6a2d217d1b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      Category|            Descript|               words|            filtered|            features|label|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|      WARRANTS|      WARRANT ARREST|   [warrant, arrest]|   [warrant, arrest]|(809,[17,32],[1.0...|  7.0|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
      "|OTHER OFFENSES|TRAFFIC VIOLATION...|[traffic, violati...|[traffic, violati...|(809,[11,17,35],[...|  1.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
      "| LARCENY/THEFT|GRAND THEFT FROM ...|[grand, theft, fr...|[grand, theft, fr...|(809,[0,2,3,4,6],...|  0.0|\n",
      "+--------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the pipeline to training documents.\n",
    "\n",
    "pipeline_fit = pipeline.fit(data)\n",
    "dataset = pipelineFit.transform(data)\n",
    "dataset.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4b1e8d3a-0710-42a0-99e3-c238e83ac950",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Sample Size: 614891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 77:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Dataset Sample Size: 263158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# split dataset into training and test sets\n",
    "\n",
    "# set seed for reproducibility\n",
    "\n",
    "(training_data, test_data) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Sample Size: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Sample Size: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f8f0cc7-23ef-4ecf-92f6-ee8a324bfd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Our model will make predictions and score on the test set; \n",
    "# we then look at the top 10 predictions from the highest probability.\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# fit the classification model\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "cls_model = lr.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c4b45f4e-8f7d-4f1f-8ee0-a034f4842113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 102:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|                      Descript|     Category|                   probability|label|prediction|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8720678987144937,0.02043...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8720678987144937,0.02043...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8720678987144937,0.02043...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8720678987144937,0.02043...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8720678987144937,0.02043...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8720678987144937,0.02043...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8720678987144937,0.02043...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, NO SE...|LARCENY/THEFT|[0.8720678987144937,0.02043...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, SERIA...|LARCENY/THEFT|[0.8720671222018433,0.02043...|  0.0|       0.0|\n",
      "|THEFT, BICYCLE, <$50, SERIA...|LARCENY/THEFT|[0.8720671222018433,0.02043...|  0.0|       0.0|\n",
      "+------------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# infer the model\n",
    "\n",
    "predictions = lrModel.transform(test_data)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa8c3729-f701-4e92-b3aa-93388a7f65d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9725533704587821"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the model\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a5cb2-05ab-437d-b600-d32f82e7430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The accuracy is excellent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e304b231-d05c-4b95-92c9-f880aa86557f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "724a0258-08d8-4486-879b-4be68d7c1845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/15 11:27:53 WARN MemoryStore: Not enough space to cache rdd_287_0 in memory! (computed 32.0 MiB so far)\n",
      "22/05/15 11:27:53 WARN BlockManager: Persisting block rdd_287_0 to disk instead.\n",
      "22/05/15 11:27:53 WARN MemoryStore: Not enough space to cache rdd_287_3 in memory! (computed 32.0 MiB so far)\n",
      "22/05/15 11:27:53 WARN BlockManager: Persisting block rdd_287_3 to disk instead.\n",
      "22/05/15 11:27:54 WARN MemoryStore: Not enough space to cache rdd_287_2 in memory! (computed 32.0 MiB so far)\n",
      "22/05/15 11:27:54 WARN BlockManager: Persisting block rdd_287_2 to disk instead.\n",
      "22/05/15 11:27:55 WARN MemoryStore: Not enough space to cache rdd_287_1 in memory! (computed 48.1 MiB so far)\n",
      "22/05/15 11:27:55 WARN BlockManager: Persisting block rdd_287_1 to disk instead.\n",
      "22/05/15 11:28:04 WARN MemoryStore: Not enough space to cache rdd_287_3 in memory! (computed 163.6 MiB so far)\n",
      "22/05/15 11:28:04 WARN MemoryStore: Not enough space to cache rdd_287_0 in memory! (computed 72.2 MiB so far)\n",
      "22/05/15 11:28:07 WARN MemoryStore: Not enough space to cache rdd_287_1 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:28:07 WARN MemoryStore: Not enough space to cache rdd_287_2 in memory! (computed 72.2 MiB so far)\n",
      "22/05/15 11:28:31 WARN MemoryStore: Not enough space to cache rdd_287_1 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:28:31 WARN MemoryStore: Not enough space to cache rdd_287_2 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:28:31 WARN MemoryStore: Not enough space to cache rdd_287_3 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:28:31 WARN MemoryStore: Not enough space to cache rdd_287_0 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:29:01 WARN MemoryStore: Not enough space to cache rdd_287_3 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:29:01 WARN MemoryStore: Not enough space to cache rdd_287_1 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:29:01 WARN MemoryStore: Not enough space to cache rdd_287_0 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:29:01 WARN MemoryStore: Not enough space to cache rdd_287_2 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:29:36 WARN MemoryStore: Not enough space to cache rdd_287_3 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:29:36 WARN MemoryStore: Not enough space to cache rdd_287_2 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:29:36 WARN MemoryStore: Not enough space to cache rdd_287_1 in memory! (computed 108.5 MiB so far)\n",
      "22/05/15 11:29:36 WARN MemoryStore: Not enough space to cache rdd_287_0 in memory! (computed 108.5 MiB so far)\n",
      "[Stage 117:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------+------------------------------+-----+----------+\n",
      "|                    Descript|     Category|                   probability|label|prediction|\n",
      "+----------------------------+-------------+------------------------------+-----+----------+\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "|PETTY THEFT FROM LOCKED AUTO|LARCENY/THEFT|[0.6111733413439591,0.06626...|  0.0|       0.0|\n",
      "+----------------------------+-------------+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Try out other classifiers - RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 100, \\\n",
    "                            maxDepth = 4, \\\n",
    "                            maxBins = 32)\n",
    "\n",
    "# Train model with Training Data\n",
    "rf_model = rf.fit(training_data)\n",
    "predictions = rf_model.transform(test_data)\n",
    "predictions.filter(predictions['prediction'] == 0) \\\n",
    "    .select(\"Descript\",\"Category\",\"probability\",\"label\",\"prediction\") \\\n",
    "    .orderBy(\"probability\", ascending=False) \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c171a438-6d5b-4daa-9bde-b2c1b6015b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6686938240895559"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eva\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6999e116-63d0-4011-af9e-91cc93554b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/15 11:40:59 WARN BlockManager: Asked to remove block broadcast_954, which does not exist\n",
      "22/05/15 11:46:19 WARN BlockManager: Asked to remove block broadcast_1471_piece0, which does not exist\n",
      "22/05/15 11:46:19 WARN BlockManager: Asked to remove block broadcast_1471, which does not exist\n",
      "22/05/15 11:46:53 WARN BlockManager: Asked to remove block broadcast_1536_piece0, which does not exist\n",
      "22/05/15 11:46:53 WARN BlockManager: Asked to remove block broadcast_1536, which does not exist\n",
      "22/05/15 11:53:43 WARN BlockManager: Asked to remove block broadcast_2155, which does not exist\n",
      "[Stage 1368:==============>                                         (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.28 s, sys: 1.91 s, total: 8.19 s\n",
      "Wall time: 25min 50s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9919757517712569"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fine tuning the better model - LogisticRegression model\n",
    "\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.1, 0.3, 0.5]) # regularization parameter\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.1, 0.2]) # Elastic Net Parameter (Ridge = 0)\n",
    "#            .addGrid(model.maxIter, [10, 20, 50]) #Number of iterations\n",
    "#            .addGrid(idf.numFeatures, [10, 100, 1000]) # Number of features\n",
    "             .build())\n",
    "# Create 5-fold CrossValidator\n",
    "cv = CrossValidator(estimator=lr, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=5)\n",
    "cv_model = cv.fit(training_data)\n",
    "\n",
    "predictions = cv_model.transform(test_data)\n",
    "\n",
    "# Evaluate best model\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23be26a4-b6a3-4297-b697-145b6e5726f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model's accuracy improves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238344a1-919b-44e5-8462-959b47e9345a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.format(\"libsvm\").load(\"data/mllib/sample_kmeans_data.txt\")\n",
    "\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(2).setSeed(1)\n",
    "model = kmeans.fit(dataset)\n",
    "\n",
    "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
    "wssse = model.computeCost(dataset)\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
